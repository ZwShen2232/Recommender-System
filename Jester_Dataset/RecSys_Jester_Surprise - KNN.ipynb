{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Standard Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Surprise library\n",
    "from surprise import Reader, Dataset, accuracy, SVD, NMF, KNNBasic, KNNWithMeans\n",
    "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Datasets\n",
    "jester_df = pd.read_csv(r\"jester_ratings.csv\")\n",
    "jester_text_df = pd.read_csv(r\"jester_items.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code with Surprise and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearch\n",
    "#parameters = {\"name\": [\"cosine\", \"pearson\"],\n",
    "#              \"user_based\": [True, False],\n",
    "#              \"min_support\": [True, False],\n",
    "#              \"min_k\": [2, 5, 10]}\n",
    "#gridknn = GridSearchCV(KNNBasic, param_grid=parameters, n_jobs=-1)\n",
    "#gridknn.fit(data)\n",
    "#print(gridknn.best_score)\n",
    "#print(gridknn.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "reader = Reader(rating_scale=(-10, 10))\n",
    "data = Dataset.load_from_df(jester_df[['userId', 'jokeId', 'rating']][:200000], reader) # using only 200k rows for faster run time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a KNNBasic model to the training set\n",
    "knn = KNNBasic(sim_options={\"name\": 'pearson',\n",
    "                            \"user_based\": True,\n",
    "                            \"min_support\": True,\n",
    "                            \"min_k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Fitting KNNBasic model to training set\n",
    "knn.fit(trainset)\n",
    "\n",
    "# Test the model on the testing set\n",
    "predictions = knn.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 3213       item: 59         r_ui = 3.25   est = 0.10   {'actual_k': 40, 'was_impossible': False}\n",
      "user: 1555       item: 117        r_ui = -5.44   est = 0.98   {'actual_k': 40, 'was_impossible': False}\n",
      "user: 1283       item: 35         r_ui = 8.41   est = 4.36   {'actual_k': 40, 'was_impossible': False}\n",
      "user: 6093       item: 143        r_ui = 1.28   est = 3.01   {'actual_k': 40, 'was_impossible': False}\n",
      "user: 6172       item: 63         r_ui = 3.16   est = 4.28   {'actual_k': 40, 'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "# Printing users actual ratings and the predicted ratings\n",
    "for prediction in predictions[0:5]:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.3557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.355720829049903"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the performance metrics\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiration from https://www.kaggle.com/code/laowingkin/netflix-movie-recommendation\n",
    "# Defining a function to recommend jokes to an user\n",
    "def recommend_KNN(userId, num_recommendations):\n",
    "    user_df = jester_text_df.copy()\n",
    "    user_df = user_df.reset_index()\n",
    "\n",
    "    data = Dataset.load_from_df(jester_df[['userId', 'jokeId', 'rating']][:200000], reader)\n",
    "\n",
    "    trainset = data.build_full_trainset()\n",
    "    knn.fit(trainset)\n",
    "\n",
    "    user_df['Estimate_Score'] = user_df['jokeId'].apply(lambda x: knn.predict(userId, x).est)\n",
    "\n",
    "    user_df = user_df.drop('jokeId', axis = 1)\n",
    "\n",
    "    user_df = user_df.sort_values('Estimate_Score', ascending=False)\n",
    "    print(user_df.head(num_recommendations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "     index                                           jokeText  Estimate_Score\n",
      "16      16  How many men does it take to screw in a light ...        5.168930\n",
      "12      12  They asked the Japanese visitor if they have e...        4.744469\n",
      "26      26  Clinton returns from a vacation in Arkansas an...        4.057151\n",
      "131    131  Mickey Mouse is having a nasty divorce with Mi...        4.043273\n",
      "34      34  An explorer in the deepest Amazon suddenly fin...        4.001111\n",
      "31      31  A man arrives at the gates of heaven. St. Pete...        3.977635\n",
      "88      88  A radio conversation of a US naval \\nship with...        3.917150\n",
      "125    125  A Briton, a Frenchman and a Russian are viewin...        3.805353\n",
      "47      47  The graduate with a Science degree asks, \"Why ...        3.764531\n",
      "104    104  A couple of hunters are out in the woods in th...        3.702548\n"
     ]
    }
   ],
   "source": [
    "# Recommend user 100 with the top 10 jokes\n",
    "recommend_KNN(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function where the user can input their user id and the number of recommendation they need\n",
    "def recommend_KNN_input():\n",
    "    userId = int(input(\"What is your User ID?\"))\n",
    "    num_recommendations = int(input(\"How many recommendations do you want?\"))\n",
    "    user_df = jester_text_df.copy()\n",
    "    user_df = user_df.reset_index()\n",
    "\n",
    "    data = Dataset.load_from_df(jester_df[['userId', 'jokeId', 'rating']][:200000], reader)\n",
    "\n",
    "    trainset = data.build_full_trainset()\n",
    "    knn.fit(trainset)\n",
    "\n",
    "    user_df['Estimate_Score'] = user_df['jokeId'].apply(lambda x: knn.predict(userId, x).est)\n",
    "\n",
    "    user_df = user_df.drop('jokeId', axis = 1)\n",
    "\n",
    "    user_df = user_df.sort_values('Estimate_Score', ascending=False)\n",
    "    print(user_df.head(num_recommendations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function where the user gets a random joke and depending if they like or dislike the joke, they get another similar joke\n",
    "import random\n",
    "\n",
    "def recommend_KNN_input_2():\n",
    "    user_df = jester_text_df.copy()\n",
    "    user_df = user_df.reset_index()\n",
    "\n",
    "    data = Dataset.load_from_df(jester_df[['userId', 'jokeId', 'rating']][:200000], reader)\n",
    "\n",
    "    trainset = data.build_full_trainset()\n",
    "    knn.fit(trainset)\n",
    "\n",
    "    # Randomly select a joke\n",
    "    random_jokeId = random.randint(1, 128)\n",
    "    random_joke_text = user_df.loc[user_df['jokeId'] == random_jokeId, 'jokeText'].item()\n",
    "    print(\"Here is your joke: \")\n",
    "    print(random_joke_text)\n",
    "\n",
    "    accum = 0\n",
    "   \n",
    "    while True:\n",
    "        liked = input(\"Do you want a similair joke? Answer with Y or N\")\n",
    "        # Ask the user to continue or leave\n",
    "        if liked.lower() == 'y':\n",
    "            user_df['Estimate_Score'] = user_df['jokeId'].apply(lambda x: knn.predict(random_jokeId, x).est)\n",
    "            user_df = user_df.sort_values('Estimate_Score', ascending=False)\n",
    "            print(\"Here is another joke: \")\n",
    "            print(user_df['jokeText'][accum])\n",
    "            accum += 1\n",
    "        else:\n",
    "            print(\"Goodbye\")\n",
    "            break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    5.3270  5.3665  5.3391  5.3497  5.3293  5.3423  0.0145  \n",
      "MAE (testset)     4.3672  4.4099  4.3710  4.3877  4.3714  4.3815  0.0159  \n",
      "Fit time          27.82   27.83   27.67   28.07   28.02   27.88   0.15    \n",
      "Test time         33.12   32.76   33.97   33.08   33.90   33.37   0.48    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([5.32701051, 5.36645845, 5.33906857, 5.34970781, 5.32926108]),\n",
       " 'test_mae': array([4.36723535, 4.40988122, 4.37099741, 4.38770515, 4.3714317 ]),\n",
       " 'fit_time': (27.815629243850708,\n",
       "  27.83255434036255,\n",
       "  27.666017055511475,\n",
       "  28.07194185256958,\n",
       "  28.018086433410645),\n",
       " 'test_time': (33.11849093437195,\n",
       "  32.758440256118774,\n",
       "  33.96618390083313,\n",
       "  33.084569215774536,\n",
       "  33.900392055511475)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run 5-fold cross-validation and print results\n",
    "cross_validate(knn, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True) # Loading time ~10 minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
